{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "510a999e-775f-41de-9c30-c833e8c7122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "sb.set(style=\"white\", font=\"Times New Roman\", context=\"paper\", palette='bright', font_scale=1.5)\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b4296e-7764-4d03-99d5-829a95fc0bb9",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b714f-4249-4de4-8038-a5dc89ef9ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cv_training(model, x_train, y_train, kpi_features, target_features, cv_method=None, k_folds=10, shuffle=False, kfold_random_state=150, *args, **kwargs):\n",
    "    \"\"\"\n",
    "        This function perform two different cross validation on training.\n",
    "        1. Leave One out:                leave_one_out\n",
    "        2. KFold :                       kfold\n",
    "        3. Regular training (Default):   None\n",
    "\n",
    "        user must give sklearn model object\n",
    "    :param model:      Sklearn model object\n",
    "    :param x_train:       the input data for trining\n",
    "    :param y_train:       the target data fpr training\n",
    "    :param kpi_features:       List of kpi features\n",
    "    :param target_features:       list of target features\n",
    "    :param cv_method:  the method of traininf. default k_fold with fold1=0\n",
    "    :param k_folds: The numer of folds for k-fold.\n",
    "    :params kfold_random_state   random state to do same k-fold\n",
    "    :param args:\n",
    "    :param kwargs:\n",
    "    :return:\n",
    "        Trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    if cv_method == \"kfold\":\n",
    "        kf = KFold(n_splits=k_folds, shuffle=shuffle, random_state=kfold_random_state)\n",
    "        for train_set, test_set in kf.split(x_train):\n",
    "            model.fit(\n",
    "                x_train[train_set, :],\n",
    "                y_train[train_set, :].ravel(),\n",
    "            )\n",
    "\n",
    "    elif cv_method == \"leave_one_out\":\n",
    "        cv = LeaveOneOut()\n",
    "        for train_set, test_set in cv.split(x_train):\n",
    "            model.fit(\n",
    "                x_train[train_set, :],\n",
    "                y_train[train_set, :].ravel(),\n",
    "            )\n",
    "    elif cv_method == \"single_shot\":\n",
    "        cv = LeaveOneOut()\n",
    "        for test_set, train_set in cv.split(x_train):  # I just changed the train_set and test_set indexes.\n",
    "            model.fit(\n",
    "                x_train[train_set, :],\n",
    "                y_train[train_set, :].ravel(),\n",
    "            )\n",
    "    else:   # regular training without any cross validation.\n",
    "        model.fit(\n",
    "            x_train,\n",
    "            y_train.ravel(),\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "def multiple_training(predictor_model, xtrain, ytrain, xtest, ytest, kpi_features, target_features, number_of_iterations=100, kfold=True, shuffle=False, kfold_random_state=150, *args, **kwargs):\n",
    "    \"\"\"\n",
    "        This function train model multiple times and return its best one.\n",
    "        It also save it.\n",
    "    :param predictor_model:   An in stnace of sklearn model\n",
    "    xtrain, ytrain, xtest, ytest: train test numpy datasets.. scaled\n",
    "    :param number_of_iterations: The numbe rof training times.\n",
    "    :param transfer_learning:  If True, it will change some directories.\n",
    "      # TODO: check later for target encoder too.\n",
    "    :param args:\n",
    "    :param kwargs:\n",
    "    :return:\n",
    "        Best trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    if kfold:\n",
    "        cv_method = \"kfold\"\n",
    "    else:\n",
    "        cv_method = None\n",
    "\n",
    "    model_performance = []\n",
    "    for i in range(number_of_iterations):\n",
    "        try:\n",
    "\n",
    "            fake_model = [predictor_model]\n",
    "            model = copy.deepcopy(fake_model)[0]\n",
    "            \n",
    "            model = cv_training(model=model, x_train=xtrain, y_train=ytrain.reshape(-1, 1), \n",
    "                    kpi_features=kpi_features, target_features=target_feature, \n",
    "                                cv_method=cv_method, \n",
    "                                k_folds=kfold, \n",
    "                                shuffle=shuffle, \n",
    "                                kfold_random_state=kfold_random_state)\n",
    "\n",
    "            model_performance.append(\n",
    "                {\n",
    "                    \"iteration\": i,\n",
    "                    \"model_instance\": model,\n",
    "                    \"test_r_squared\": round(\n",
    "                        r2_score(\n",
    "                            ytest.reshape(-1, 1),\n",
    "                            model.predict(xtest).reshape(-1, 1)\n",
    "                        ),\n",
    "                        3\n",
    "                    ),\n",
    "                    \"train_r_squared\": round(\n",
    "                        r2_score(\n",
    "                            ytrain.reshape(-1, 1),\n",
    "                            model.predict(xtrain).reshape(-1, 1)\n",
    "                        ),\n",
    "                        3\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(f\"Model: {model}, Iteration: {i}, Train R squared: {model_performance[i]['train_r_squared']}, Test R squared: {model_performance[i]['test_r_squared']}\")\n",
    "            del model, fake_model\n",
    "        except KeyboardInterrupt as e:\n",
    "            print(\"\\nProcess terminated by user ...!\")\n",
    "            break\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(model_performance)), [j[\"train_r_squared\"] for j in model_performance], color=\"red\")\n",
    "    plt.plot(range(len(model_performance)), [j[\"test_r_squared\"] for j in model_performance], color=\"blue\")\n",
    "    plt.title(\"Train and Test $R^2$\")\n",
    "    plt.xlabel(\"Training Iteration\")\n",
    "    plt.ylabel(\"$R^2$\")\n",
    "    plt.legend([f\"Train $R^2$, Mean:{round(np.mean([i['train_r_squared'] for i in model_performance]), 2)}\", f\"Test $R^2$, Mean:{round(np.mean([i['test_r_squared'] for i in model_performance]), 2)}\"])\n",
    "    \n",
    "    # if not transfer_learning:\n",
    "    #     plt.savefig(os.path.join(model_instance.model_dir, f\"{model_instance.model_name}_r_squared.png\"), dpi=300)\n",
    "    # else:\n",
    "    #     plt.savefig(os.path.join(model_instance.model_dir, f\"after_transfer_learning/{model_instance.model_name}_r_squared.png\"), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Get model with hghiest Test data R squared.\n",
    "    model = sorted(model_performance, key=itemgetter(\"test_r_squared\"), reverse=True)[0][\"model_instance\"]\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e3829-e034-4e64-9579-7b82384206ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_reporter(y_pred, y_true, *args, **kwargs):\n",
    "    \n",
    "    print(\"R2:  \", metrics.r2_score(y_true=y_true, y_pred=y_pred))\n",
    "    print(\"RMSE:  \", metrics.mean_squared_error(y_true=y_true, y_pred=y_pred, squared=False))\n",
    "    print(\"MAE:  \", metrics.mean_absolute_error(y_true=y_true, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef70136f-a373-4d62-afd6-4335088c4017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# ## Material Properties: For compounds I used harmonic average.\n",
    "metal_work_functions = {\n",
    "    \"Co\": 4.52, \"Cr\": 4.11, \"Ir\": 5.31, \"NiO\": 2.49, \n",
    "    \"Au\": 5.06, \"Cu\": 4.53, \"Ni\": 4.98, \"Pd\": 5.06, \n",
    "    \"Pt\": 5.54, \"Rh\": 4.99, \"Ag\": 4.22, \"Bare\": 0,\n",
    "    \"nan\": np.NaN\n",
    "}\n",
    "metal_atomic_numbers = {\n",
    "    \"Co\": 27, \"Cr\": 24, \"Ir\": 77, \"NiO\": 18, \"Au\": 79,\n",
    "    \"Cu\": 29, \"Ni\": 28, \"Pd\": 46, \"Pt\": 78, \"Rh\": 45,\n",
    "    \"Ag\": 47, \"Bare\": 0,\n",
    "    \"nan\": np.NaN\n",
    "}\n",
    "metal_electronegativity = {\n",
    "    \"Co\": 1.88, \"Cr\": 1.66, \"Ir\": 2.20, \"NiO\": 2.675,\n",
    "    \"Au\": 2.54, \"Cu\": 1.9, \"Ni\": 1.91, \"Pd\": 2.2,\n",
    "    \"Pt\": 2.28, \"Rh\": 2.28, \"Ag\": 1.93, \"Bare\": 0,\n",
    "    \"nan\": np.NaN\n",
    "}\n",
    "\n",
    "engineered_features = pd.read_excel(\"../../alcohol_structure/engineered_features.xlsx\")\n",
    "\n",
    "def value_to_cat(df, list_of_alcohol):\n",
    "    df['MT'] = \"\"    # Metal atomic number\n",
    "    df['AT'] = \"\"    # alcohol molecular weight\n",
    "\n",
    "    for i in df.index.tolist():\n",
    "        for j, key in enumerate(metal_atomic_numbers):\n",
    "            if df.loc[i, \"MAN\"] == metal_atomic_numbers[key]:\n",
    "                df.loc[i, 'MT'] = key\n",
    "        for at in list_of_alcohol:\n",
    "            if df.loc[i, \"ATI\"] == engineered_features[engineered_features.name == at].L_pca_feature.to_numpy()[0]:\n",
    "                df.loc[i, 'AT'] = at\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb5f2e7-9b70-4b80-bfd6-4d99f7fd369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from numpy.random import rand\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "class BayesianOptimizer():\n",
    "    \"\"\"\n",
    "        This class perform gradient descent algorithm on optimization for single objective\n",
    "\n",
    "        This class perform genetic algorithm for single sample.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(self, model, kpi_features, target_feature, optimization_features, optimization_rate=0.01, threshold=1e-3, decay_rate=0.9, feature_bound=[-1, 1], *args, **kwargs):\n",
    "        \"\"\"\n",
    "            This method initialize the values with objective of the optimization\n",
    "        :param model:     A model instance with .predict method.\n",
    "        :param database:  A database includes (feature for .predict method of the model) kpi feature and target features not anything else for\n",
    "        :param kpi_features:  The ki feature for prediction.\n",
    "        :param target_feature:  The target feature of main model predictor (keras or sklearn model)\n",
    "        :param optimization_feature:       The desired feature for optimization\n",
    "        :param optimization_rate:       The gradient descent optimization rate.\n",
    "        :param threshold:       Threshold to stop iterations.\n",
    "        :param decay_rate:       The Decay rate for momentom\n",
    "        :param feature_bound:       The bound of feature to calculate the optimzation results.\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.kpi_features = kpi_features\n",
    "        self.target_feature = target_feature\n",
    "\n",
    "        self.decay_rate = decay_rate\n",
    "        self.optimization_rate = optimization_rate\n",
    "        self.epsilon = 1e-4     # use for derivative calculations.\n",
    "        self.threshold = threshold   # Stop criteria\n",
    "\n",
    "        self.feature_bound = np.array([feature_bound])\n",
    "        self.optimization_features = optimization_features\n",
    "\n",
    "    def objective_function(self, optimization_feature_value):\n",
    "        \"\"\"\n",
    "            This method is used for the genetic optimizer to predict values.\n",
    "        :param optimization_feature:    The value for the feature with optimization.\n",
    "        :return:\n",
    "            The predicted values of the main model\n",
    "        \"\"\"\n",
    "\n",
    "        input_data = self.database[self.kpi_features].copy()\n",
    "        input_data[self.optimization_features] = optimization_feature_value\n",
    "\n",
    "        predicted_targets = self.model.predict(np.array(input_data, dtype=float))\n",
    "        \n",
    "        # Negetive tfor maximation\n",
    "        predicted_targets = -1 * predicted_targets\n",
    "        return predicted_targets\n",
    "\n",
    "    def run_optimization(self, database, *args, **kwargs):\n",
    "        \"\"\"\n",
    "            This method on call run the optimization loop.\n",
    "        :param database:    The input database for optimization (single row ?)\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "            The optimization model.\n",
    "        \"\"\"\n",
    "        self.database = database\n",
    "\n",
    "        # initial guess:\n",
    "        # solution = self.feature_bound[:, 0] + rand(len(self.feature_bound)) * (self.feature_bound[:, 1] - self.feature_bound[:, 0])\n",
    "        \n",
    "        solution = self.database.iloc[0][self.optimization_features].values.reshape(1, -1)\n",
    "        \n",
    "        objective_function = 1\n",
    "        count = 0\n",
    "        diff = 0\n",
    "        while objective_function > self.threshold:\n",
    "\n",
    "            try:\n",
    "                # Update guess\n",
    "                gradient = self.gradient(optimization_feature_value=solution)\n",
    "                diff = self.decay_rate * diff - self.optimization_rate * gradient\n",
    "                solution = solution + diff\n",
    "\n",
    "                # Calculate the objective function\n",
    "                objective_function = self.objective_function(optimization_feature_value=solution)\n",
    "\n",
    "                count += 1\n",
    "                print(f\"Iteration Number:  {count},  Cost Function: {objective_function}, Update: {diff[0]}\")\n",
    "                if (count > 50) or np.abs(diff) < self.threshold:\n",
    "                    break\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Optimization terminated by user!\")\n",
    "                break\n",
    "        return solution, objective_function, count\n",
    "\n",
    "    def gradient(self, optimization_feature_value):\n",
    "        \"\"\"\n",
    "            This function calculate gradient of the predictor model and return the value\n",
    "        :return:\n",
    "            Gradient value of the predictor model.\n",
    "        \"\"\"\n",
    "\n",
    "        input_data = self.database[self.kpi_features].copy()\n",
    "        input_data[self.optimization_features] = optimization_feature_value\n",
    "\n",
    "        # calculate Gradient\n",
    "        x_add_delta_x = input_data[self.kpi_features].copy()\n",
    "        x_add_delta_x[self.optimization_features] += self.epsilon\n",
    "\n",
    "        x_mines_delta_x = input_data[self.kpi_features].copy()\n",
    "        x_mines_delta_x[self.optimization_features] += - self.epsilon\n",
    "\n",
    "        return (self.model.predict(np.array(x_add_delta_x, dtype=float)) - self.model.predict(np.array(x_mines_delta_x, dtype=float))) / (2 * self.epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8dd5bc-0be2-4cae-91b1-84207664b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin\n",
    "from math import pi\n",
    "from numpy import arange\n",
    "from numpy import vstack\n",
    "from numpy import argmax\n",
    "from numpy import asarray\n",
    "from numpy.random import normal\n",
    "from numpy.random import random\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from warnings import catch_warnings\n",
    "from warnings import simplefilter\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "# objective function\n",
    "def objective(x, noise=0.1):\n",
    "    return -1 * model.predict(x)\n",
    "\n",
    "# surrogate or approximation for the objective function\n",
    "def surrogate(gpr_model, X):\n",
    "    # catch any warning generated when making a prediction\n",
    "    with catch_warnings():\n",
    "        # ignore generated warnings\n",
    "        simplefilter(\"ignore\")\n",
    "        return gpr_model.predict(X, return_std=True)\n",
    "\n",
    "\n",
    "# probability of improvement acquisition function\n",
    "def acquisition(X, Xsamples, gpr_model):\n",
    "    # calculate the best surrogate score found so far\n",
    "    yhat, _ = surrogate(gpr_model, X)\n",
    "    best = max(yhat)\n",
    "    # calculate mean and stdev via surrogate function\n",
    "    mu, std = surrogate(gpr_model, Xsamples)\n",
    "\n",
    "    mu = mu.reshape(-1, )\n",
    "    # calculate the probability of improvement\n",
    "    probs = norm.cdf((mu - best) / (std+1E-9))\n",
    "    return probs\n",
    "\n",
    "\n",
    "# optimize the acquisition function\n",
    "def opt_acquisition(X, y, gpr_model, Xsamples):\n",
    "    # random search, generate random samples\n",
    "    # Xsamples = random(100)\n",
    "    # Xsamples = Xsamples.reshape(len(Xsamples), 1)\n",
    "    # calculate the acquisition function for each sample\n",
    "    scores = acquisition(X, Xsamples, gpr_model)\n",
    "\n",
    "    # locate the index of the largest scores\n",
    "    ix = argmax(scores)\n",
    "    return Xsamples[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c4f8d-3e37-4df7-8a8e-1acb18521c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8702ee03-2c76-45b5-8aaf-7aab6d56b1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "520e1510-d3bd-4ac7-9d30-385109eba916",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2693f4e4-99e1-4be7-954a-12f49e5d06e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_excel(\"path to all_data_file.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7969ea6e-287b-4f3c-be1f-1447f0fdc40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature for alcohol type.\n",
    "engineered_features = pd.read_excel(\"path to engineered_features.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d01a8de-04ff-4940-8c50-1f63e340f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain models for other LI features change it based on the name of the feature:\n",
    "# ApP, LI, AcP, and None.\n",
    "\n",
    "kpi_features = [\"ML\", \"AcP\", \"AC\", \"MWF\", \"ATI\"]\n",
    "target_feature = [\"Rate\"]            # Active photons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d5d421-43fb-4669-afa6-b8d6188927ae",
   "metadata": {},
   "source": [
    "# preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93a3122-10fe-4174-baec-f0b701a4b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scaling.\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "input_data = all_data[kpi_features].copy()\n",
    "target_data = all_data[target_feature].copy().values.reshape(-1, 1)\n",
    "\n",
    "input_scaler = preprocessing.StandardScaler(with_mean=True, with_std=True)\n",
    "target_scaler = preprocessing.StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "# input_scaler = preprocessing.MinMaxScaler((0, 1))\n",
    "# target_scaler = preprocessing.MinMaxScaler((0, 1))\n",
    "\n",
    "input_scaler.fit(input_data)\n",
    "target_scaler.fit(target_data)\n",
    "normal_all_data = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(input_scaler.transform(input_data), columns=input_data.columns.tolist()),\n",
    "        pd.DataFrame(target_scaler.transform(target_data), columns=target_feature)\n",
    "    ],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd58508-b1a0-4d47-9df5-8c4548caf855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e9ffe20-a1c4-4a27-b8c6-7dd492a7cf70",
   "metadata": {},
   "source": [
    "# Active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc94c70-6ae3-433d-abc3-92ae9c8b1bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import kernels\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "training_data = all_data.sample(10, random_state=365).copy()\n",
    "test_data = all_data[~all_data.index.isin(training_data.index.values.tolist())].copy()\n",
    "\n",
    "xtrain, ytrain = input_scaler.transform(training_data[kpi_features]), target_scaler.transform(training_data[target_feature].values)\n",
    "xtest, ytest = input_scaler.transform(test_data[kpi_features]), target_scaler.transform(test_data[target_feature].values)\n",
    "\n",
    "\n",
    "all_rmse = []\n",
    "all_mstde = []\n",
    "id_of_selected_samples = []\n",
    "\n",
    "\n",
    "# test_ddd = value_to_cat(test_data.copy(), list_of_alcohol=lit_database.AT.value_counts().index.tolist())\n",
    "training_data_ = training_data.copy()\n",
    "test_data_ = test_data.copy()\n",
    "\n",
    "counts = -1\n",
    "while True:\n",
    "    # my_kernel = 0.5 * kernels.ConstantKernel(0.5, (1e-23, 1e20)) + \\\n",
    "    #         kernels.RBF(0.9, (1e-23, 1e20)) * kernels.RBF(0.9, (1e-23, 1e20)) + \\\n",
    "    #         kernels.WhiteKernel(0.1, (1e-23, 1e20)) \n",
    "\n",
    "\n",
    "    # gpr_instance = GaussianProcessRegressor(\n",
    "    #     kernel=my_kernel, \n",
    "    #     alpha=1e-10,\n",
    "    #     normalize_y=True,\n",
    "    #     copy_X_train=False,\n",
    "    #     random_state=20,\n",
    "    # )\n",
    "    \n",
    "    gpr_instance = RandomForestRegressor(n_estimators=1500, max_depth=800)\n",
    "\n",
    "\n",
    "    gpr = cv_training(model=gpr_instance, x_train=xtrain, y_train=ytrain.reshape(-1, 1), kpi_features=kpi_features, \n",
    "                        target_features=target_feature, cv_method=\"kfold\", k_folds=5, shuffle=True)\n",
    "\n",
    "    counts+=1\n",
    "    print(f\"\\nIteration {counts} ================\")\n",
    "    print(\"Training Score: \", gpr.score(xtrain, ytrain), \"\\n\")\n",
    "    print(\"Test Score: \", gpr.score(xtest, ytest))\n",
    "\n",
    "\n",
    "\n",
    "    # Both model:\n",
    "    # ypredtrain, stdpredtrain = target_scaler.inverse_transform(gpr.predict(xtrain, return_std=True)[0].reshape(-1, 1)), target_scaler.inverse_transform(gpr.predict(xtrain, return_std=True)[1].reshape(-1, 1))\n",
    "    # ypredtest, stdpredtest = target_scaler.inverse_transform(gpr.predict(xtest, return_std=True)[0].reshape(-1, 1)), target_scaler.inverse_transform(gpr.predict(xtest, return_std=True)[1].reshape(-1, 1))\n",
    "    \n",
    "    ypredtrain = target_scaler.inverse_transform(gpr.predict(xtrain).reshape(-1, 1))\n",
    "    ypredtest = target_scaler.inverse_transform(gpr.predict(xtest).reshape(-1, 1))\n",
    " \n",
    "\n",
    "    # print(\"MSTDE\", np.mean(stdpredtest))\n",
    "    print(\"RMSE\", metrics.mean_squared_error(\n",
    "        y_pred=ypredtest, y_true=test_data_[target], squared=False))\n",
    "    \n",
    "    all_rmse.append(metrics.mean_squared_error(\n",
    "        y_pred=ypredtest, y_true=test_data_[target], squared=False))\n",
    "    # all_mstde.append(np.mean(stdpredtest))\n",
    "\n",
    "\n",
    "\n",
    "    test_errors = ypredtest - target_scaler.inverse_transform(ytest)\n",
    "\n",
    "    _test_data = pd.DataFrame(columns=[\"error\", \"prediction\", \"abs_error\", \"std_of_error\"], \n",
    "                              index=test_data_.index.tolist())\n",
    "    _test_data[\"error\"] = test_errors.reshape(-1, 1)\n",
    "    _test_data[\"prediction\"] = ypredtest.reshape(-1, 1)\n",
    "    _test_data[\"abs_error\"] = abs(test_errors.reshape(-1, 1)).reshape(-1, 1) \n",
    "    _test_data[\"std_of_error\"] = np.NaN #stdpredtest.reshape(-1, 1)\n",
    "\n",
    "    _test_data.index.name = \"ID\"\n",
    "    _test_data = pd.concat([_test_data, test_data_], axis=1)\n",
    "    \n",
    "    # Check if continue\n",
    "    # if not int(input(\"Enter 1 to pick up the 20 sample of test data \\nwhihth hiest error and train a new model\\nor enter zero to stop active learning:  \")):\n",
    "    #     break\n",
    "    if len(test_data_) <= 70:\n",
    "        break\n",
    "    # if gpr.score(xtest, ytest) > gpr.score(xtrain, ytrain):\n",
    "    #     break\n",
    "\n",
    "    _test_data.sort_values(by=\"abs_error\", ascending=False, inplace=True)\n",
    "    samples_with_highest_abs_error = _test_data.iloc[:10].copy()\n",
    "    \n",
    "    print(\"selected samples: \\n\", samples_with_highest_abs_error.index.values.tolist())\n",
    "    \n",
    "    id_of_selected_samples.append(samples_with_highest_abs_error.index.values.tolist())\n",
    "    \n",
    "    # import pdb; pdb.set_trace()\n",
    "    \n",
    "    training_data_ = pd.concat([training_data_, samples_with_highest_abs_error[kpi_features + target_feature]], axis=0)\n",
    "    training_data_.sample(frac=1, replace=True)\n",
    "    test_data_ = test_data_[~test_data_.index.isin(samples_with_highest_abs_error.index.values.tolist())]\n",
    "    # test_ddd = value_to_cat(test_data_.copy(), list_of_alcohol=lit_database.AT.value_counts().index.tolist())\n",
    "    \n",
    "    # change x and y train and test\n",
    "    xtrain, ytrain = input_scaler.transform(training_data_[kpi_features]), target_scaler.transform(training_data_[target_feature].values)\n",
    "    xtest, ytest = input_scaler.transform(test_data_[kpi_features]), target_scaler.transform(test_data_[target_feature].values)\n",
    "\n",
    "    print(\"train size: \", ytrain.shape[0], \"   test size: \", ytest.shape[0])\n",
    "    \n",
    "plt.plot(all_rmse, color=\"red\")\n",
    "# plt.plot(all_mstde, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60377563-f8a7-4b00-8c7a-78b111ba9c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1c8d9-815c-4573-87d3-4e3728ad1ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training Randomforest tree for checking the trees, \n",
    "\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "\n",
    "xtrain, ytrain = input_scaler.transform(training_data[kpi_features]), training_data[target_feature].values\n",
    "xtest, ytest = input_scaler.transform(validation_data[kpi_features]), validation_data[target_feature].values\n",
    "\n",
    "\n",
    "xg_model = XGBRegressor(random_state=200)\n",
    "# xg_model = XGBRFRegressor(learning_rate=0.04, n_estimators=100, max_depth=40, random_state=200)\n",
    "xg_model = cv_training(model=xg_model, x_train=xtrain, y_train=ytrain.reshape(-1, 1), kpi_features=kpi_features, \n",
    "                    target_features=target_feature, cv_method=\"kfold\", k_folds=5, shuffle=True)\n",
    "\n",
    "print(\"Train: \")\n",
    "metric_reporter(\n",
    "    y_pred=xg_model.predict(input_scaler.transform(training_data[kpi_features])).reshape(-1, 1),\n",
    "    y_true=training_data[target_feature], \n",
    ")\n",
    "\n",
    "print(\"Test: \")\n",
    "metric_reporter(\n",
    "    y_pred=xg_model.predict(input_scaler.transform(validation_data[kpi_features])).reshape(-1, 1),\n",
    "    y_true=validation_data[target_feature], \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d629f2-710c-4a65-a113-1d45ccb558ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ffaca0-79a7-4ab0-b772-41b53fded981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e962ef90-9ed0-44fb-9e2b-6a8ee1c6b02c",
   "metadata": {},
   "source": [
    "# TPOT Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb210e-a2e2-4226-ad09-aba96c671483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpot\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "xtrain, ytrain = input_scaler.transform(training_data[kpi_features]), target_scaler.transform(training_data[target_feature].values)\n",
    "\n",
    "model = tpot.TPOTRegressor(\n",
    "    generations=5, \n",
    "    population_size=100, \n",
    "    scoring='neg_mean_absolute_error', \n",
    "    cv=cv, \n",
    "    verbosity=2, \n",
    "    random_state=1, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "all_data[\"predictions\"] = target_scaler.inverse_transform(model.predict(input_scaler.transform(all_data[kpi_features])).reshape(-1, 1)).reshape(-1, )\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(\n",
    "    training_data[target_feature],\n",
    "    target_scaler.inverse_transform(model.predict(input_scaler.transform(training_data[kpi_features])).reshape(-1, 1)), color=\"blue\"\n",
    ")\n",
    "plt.scatter(\n",
    "    validation_data[target_feature],\n",
    "    target_scaler.inverse_transform(model.predict(input_scaler.transform(validation_data[kpi_features])).reshape(-1, 1)), color=\"green\"\n",
    ")\n",
    "plt.plot(all_data[target_feature], all_data[target_feature], color=\"red\")\n",
    "\n",
    "plt.legend([\n",
    "    f\"Training data\", \n",
    "    f\"Validation data\"\n",
    "])# plt.title(\"EXP data\")\n",
    "plt.xlabel(f\"Actual {target_feature[0]}\")\n",
    "plt.ylabel(f\"Predicted {target_feature[0]}\")\n",
    "# plt.savefig(\"acp_scatter.png\", dpi=500, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b3052-fb3d-46ae-a28c-c26445865d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de222d07-5885-4098-b1c5-979bc102bd60",
   "metadata": {},
   "source": [
    "# Repeatibility test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e501e-66fb-4a7b-acce-31be998faf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpot\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "xtrain_, ytrain_ = input_scaler.transform(training_data[kpi_features]), target_scaler.transform(training_data[target_feature].values)\n",
    "\n",
    "repeated_predictions_ = []\n",
    "for i__ in range(2):\n",
    "    \n",
    " \n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    model__ = tpot.TPOTRegressor(\n",
    "        generations=5, \n",
    "        population_size=100, \n",
    "        scoring='neg_mean_absolute_error', \n",
    "        cv=cv, \n",
    "        verbosity=0, \n",
    "        random_state=1, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model__.fit(xtrain_, ytrain_)\n",
    "    repeated_predictions_.append(model__.predict(xtrain_).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e104aa3-e044-4721-9c23-1ae4d105e610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddd24637-c071-457b-b34b-3444eb2e153c",
   "metadata": {},
   "source": [
    "# Y-scrambeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cfe744-7330-4c59-8135-f1c608e9921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpot\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "xtrain_, ytrain_ = input_scaler.transform(training_data[kpi_features]), target_scaler.transform(training_data[target_feature].values)\n",
    "\n",
    "r2_suffeled = []\n",
    "for i__ in range(100):\n",
    "    \n",
    "    np.random.shuffle(ytrain_)\n",
    "    \n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    model__ = tpot.TPOTRegressor(\n",
    "        generations=5, \n",
    "        population_size=100, \n",
    "        scoring='neg_mean_absolute_error', \n",
    "        cv=cv, \n",
    "        verbosity=0, \n",
    "        random_state=1, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model__.fit(xtrain_, ytrain_)\n",
    "    y_pred_ = model__.predict(xtrain_).reshape(-1, 1)\n",
    "    \n",
    "    r2_suffeled.append(metrics.r2_score(y_true=ytrain_, y_pred=y_pred_))\n",
    "    \n",
    "    if i__ % 10 == 0:\n",
    "        print(i__)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb8fb5-40bb-490e-ae34-611f926ee3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ab73568-4692-4148-9c5e-b37a650fbc6f",
   "metadata": {},
   "source": [
    "# Random flactuation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85d100-9d93-4ba3-aafe-71d4c37ec396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpot\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "all_r2 = []\n",
    "all_rmse = []\n",
    "all_mae = []\n",
    "\n",
    "for ii in range(100):\n",
    "    \n",
    "    cv = RepeatedKFold(\n",
    "        n_splits=10,\n",
    "        n_repeats=10,\n",
    "        random_state=np.random.randint(1000)\n",
    "    )\n",
    "    xtrain, ytrain = input_scaler.transform(training_data[kpi_features]), target_scaler.transform(training_data[target_feature].values)\n",
    "\n",
    "    model = tpot.TPOTRegressor(\n",
    "        generations=5, \n",
    "        population_size=100, \n",
    "        scoring='neg_mean_absolute_error', \n",
    "        cv=cv, \n",
    "        verbosity=0, \n",
    "        random_state=np.random.randint(1000), \n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(xtrain, ytrain)\n",
    "\n",
    "    y_hat = target_scaler.inverse_transform(model.predict(xtrain).reshape(-1, 1)).reshape(-1, 1)\n",
    "    \n",
    "    all_r2.append(metrics.r2_score(y_true=training_data[target_feature].values.reshape(-1, 1), y_pred=y_hat))\n",
    "    all_rmse.append(metrics.mean_squared_error(y_true=training_data[target_feature].values.reshape(-1, 1), y_pred=y_hat, squared=False))\n",
    "    all_mae.append(metrics.mean_absolute_error(y_true=training_data[target_feature].values.reshape(-1, 1), y_pred=y_hat))\n",
    "\n",
    "    \n",
    "    if ii % 10 == 0:\n",
    "        print(ii, \" percent passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f36200-099c-4d10-afe0-d72d92514b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ab1c44-20d3-47b9-86d0-6084ba2df259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f248b978-24c9-4ecb-a813-99cc465da7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4e01d0c-c7f8-48b4-81ed-e4c5e0b74c22",
   "metadata": {},
   "source": [
    "# SHAP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff13929e-7b91-429a-8988-a7df9f951d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape analysis of final model \n",
    "import shap\n",
    "\n",
    "explainer = shap.KernelExplainer(\n",
    "    model.predict, \n",
    "    input_scaler.transform(all_data[kpi_features]) \n",
    ")\n",
    "\n",
    "shap_values = explainer.shap_values(\n",
    "    input_scaler.transform(all_data[kpi_features]),\n",
    "    # nsamples=420,\n",
    "    # alpha=0.002\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac332787-ecdf-4392-9dae-bd2694c36806",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "shap.bar_plot(np.std(shap_values[0], axis=0), feature_names=kpi_features)\n",
    "# f.savefig(\"shape_bar_plot_acp_rate.png\", dpi=1000, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afaad19-640c-44ba-a5af-37e2d1d4e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part already been calculated and you can find it in the data folder.\n",
    "\n",
    "\n",
    "shape_value_anlaysis = {}\n",
    "for indexer, i in enumerate(kpi_features):\n",
    "    \n",
    "    mean__ = input_scaler.mean_.tolist()[indexer]\n",
    "    std__ = np.sqrt(input_scaler.var_.tolist()[indexer])\n",
    "    \n",
    "    fig, ax, x, y, hist, features, features_tmp = partial_dependence(\n",
    "        npoints=2000, \n",
    "        ind=i,\n",
    "        model=model.predict, \n",
    "        data=input_scaler.transform(all_data[kpi_features]), \n",
    "        xmin='percentile(0)',\n",
    "        xmax='percentile(100)',\n",
    "        model_expected_value=False, \n",
    "        feature_expected_value=False, \n",
    "        feature_names=kpi_features, \n",
    "        ylabel=target_feature[0], \n",
    "        show=False, ice=False, hist=False)  # shoe=False\n",
    "    \n",
    "    \n",
    "    shape_value_anlaysis[i] = {\n",
    "        # \"fig\": fig,\n",
    "        # \"ax\": ax,\n",
    "        f\"{i}\": revers_feat(x, mean=mean__, std=std__),\n",
    "        \"mean_Rate\": target_scaler.inverse_transform(y[0].reshape(-1, 1)).reshape(-1, ),\n",
    "        \"std_Rate\": target_scaler.inverse_transform(y[1].reshape(-1, 1)).reshape(-1, ),\n",
    "        \"min_Rate\": target_scaler.inverse_transform(y[2].reshape(-1, 1)).reshape(-1, ),\n",
    "        \"max_Rate\": target_scaler.inverse_transform(y[3].reshape(-1, 1)).reshape(-1, ),\n",
    "        \"median_Rate\": target_scaler.inverse_transform(y[4].reshape(-1, 1)).reshape(-1, ),\n",
    "        #\"hist\": hist,\n",
    "        #\"features\": features\n",
    "    }\n",
    "    #plt.figure()\n",
    "    #plt.scatter(ax.get_lines()[0].get_xdata(), ax.get_lines()[0].get_ydata(), s=[2]*len(ax.get_lines()[0].get_xdata()))\n",
    "    # plt.savefig(f\"shape_analysis/acp_rate/shape_partial_dependency_{i}_plot.jpg\", dpi=1000, bbox_inches=\"tight\")\n",
    "    \n",
    "    pd.DataFrame(shape_value_anlaysis[i]).to_excel(f\"C:/Users/z5326694/OneDrive - UNSW/Desktop/PhD/PhD papers/light intensity paper/paper_data/shape_partial_dependence_{i}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43efcf67-dc63-4f18-8c75-33ed9ec1e823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2735ddc-1ea8-4477-9640-8021d2ced522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e180ddd-1a5e-4cdf-9ec7-b7e53c79ab02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011edba-83ba-4b9f-9a5d-4fcbeb4a4804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "769e32bd-e357-42bf-906b-6320164d30c7",
   "metadata": {},
   "source": [
    "# Load model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5834b-b9df-47b2-922d-7ad18e470312",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import joblib\n",
    "model = joblib.load(os.path.join(\"acp_rate_model.sav\"))\n",
    "\n",
    "with open(os.path.join(\"acp_rate_model.txt\")) as f:\n",
    "    print(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168fcd4b-323e-4202-96ab-0f21a7423c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi_features = ['ML', 'AcP', 'AC', 'MWF', 'ATI']\n",
    "target_feature = ['Rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ea18e-0c03-45c8-8563-78a04e0ad077",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
